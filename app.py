import streamlit as st
import pandas as pd
import google.generativeai as genai

# --- CONFIGURA√á√ÉO IA ---
# Chave fornecida: AIzaSyCAupRudeVbP7QSSw7v4BDjG0zJ4Y5XE-0
GENAI_KEY = "AIzaSyCAupRudeVbP7QSSw7v4BDjG0zJ4Y5XE-0"
genai.configure(api_key=GENAI_KEY)

# Configura√ß√£o rigorosa do Modelo Flash para evitar Erro 404
# O prefixo 'models/' √© essencial em algumas vers√µes da biblioteca
model = genai.GenerativeModel('models/gemini-1.5-flash')

# --- CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(page_title="IST Planner GPT", layout="wide", page_icon="üéì")

@st.cache_data
def load_data():
    try:
        # Carregamento do ficheiro gerado pelo script anterior
        df = pd.read_csv("planeamento_ist_detalhado_2026.csv", encoding="utf-8-sig")
        # Garantir que IDs n√£o apare√ßam com v√≠rgulas de milhar
        df['id_cadeira'] = df['id_cadeira'].astype(str)
        return df
    except Exception as e:
        st.error(f"Erro ao carregar ficheiro CSV: {e}")
        return pd.DataFrame()

df = load_data()

# --- INTERFACE ---
st.title("üöÄ IST Smart Planner & AI Assistant")
st.markdown("---")

# Abas para separa√ß√£o de fun√ß√µes
tab_explorador, tab_chat = st.tabs(["üîç Explorador de Disciplinas", "ü§ñ Assistente IA (Chat)"])

# --- TAB 1: EXPLORADOR MANUAL ---
with tab_explorador:
    st.sidebar.header("Filtros de Procura")
    # Filtro por texto (Nome, Sigla ou ID)
    search = st.sidebar.text_input("Procurar (Ex: MEGE, Aero, 3379...)", "").lower()
    
    # Filtro por Per√≠odo
    periodos = ["Todos"] + sorted(df['periodo'].dropna().unique().tolist())
    periodo_sel = st.sidebar.selectbox("Filtrar por Per√≠odo", periodos)

    mask = (df['sigla_curso_ref'].str.lower().str.contains(search, na=False) | 
            df['nome_cadeira'].str.lower().str.contains(search, na=False) |
            df['id_cadeira'].str.contains(search, na=False))
    
    if periodo_sel != "Todos":
        mask = mask & (df['periodo'] == periodo_sel)
    
    df_filtered = df[mask]
    
    if not df_filtered.empty:
        # Formata√ß√£o de etiquetas usando pelicas conforme as prefer√™ncias
        df_filtered['label'] = "[" + df_filtered['sigla_curso_ref'] + "] " + df_filtered['nome_cadeira']
        
        escolha = st.selectbox("Selecione uma disciplina para detalhes:", ["-- Selecione --"] + sorted(df_filtered['label'].tolist()))
        
        if escolha != "-- Selecione --":
            row = df_filtered[df_filtered['label'] == escolha].iloc[0]
            
            c1, c2 = st.columns([2, 1])
            with c1:
                st.header(row['nome_cadeira'])
                st.subheader("üìñ Programa")
                st.write(row['programa'] if pd.notna(row['programa']) else "Informa√ß√£o n√£o dispon√≠vel.")
                
                st.subheader("üìù M√©todo de Avalia√ß√£o")
                st.info(row['metodo_avaliacao'] if pd.notna(row['metodo_avaliacao']) else "Detalhes n√£o especificados.")
            
            with c2:
                st.metric("Cr√©ditos ECTS", row['ects'])
                st.metric("Per√≠odo Letivo", row['periodo'])
                st.write("**üë®‚Äçüè´ Corpo Docente:**")
                docentes = str(row['docentes']).split(" | ") if pd.notna(row['docentes']) else ["N√£o listados"]
                for d in docentes:
                    st.write(f"- {d}")
                st.divider()
                st.link_button("üåê Ver no F√©nix", row['url_curso'])
    else:
        st.warning("Nenhuma disciplina encontrada com os crit√©rios atuais.")

# --- TAB 2: ASSISTENTE IA (CHAT) ---
with tab_chat:
    st.header("ü§ñ Intelig√™ncia Artificial sobre o IST")
    st.info("O assistente utiliza o modelo **Gemini 1.5 Flash** e tem acesso a toda a tabela CSV.")

    # Prepara√ß√£o do contexto: CSV √© mais eficiente que Markdown para o Flash
    # Enviamos apenas colunas essenciais para poupar tokens e manter a precis√£o
    contexto_ia = df[['sigla_curso_ref', 'nome_cadeira', 'ects', 'periodo', 'metodo_avaliacao', 'programa']].to_csv(index=False)

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Hist√≥rico de conversa√ß√£o
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Pergunte algo (ex: Quais as cadeiras de P4 com exame em MEGE?)"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            # Prompt de Engenharia para o Gemini
            full_prompt = f"""
            Tu √©s um consultor de planeamento acad√©mico do IST.
            O teu utilizador √© um Engenheiro Aeroespacial e empres√°rio. Responde de forma estruturada.
            
            Usa APENAS os dados abaixo para responder:
            {contexto_ia}
            
            Regra: Ao referir nomes de colunas como sigla_curso_ref ou nome_cadeira, usa sempre `pelicas`.
            Pergunta: {prompt}
            """
            
            try:
                # Gera√ß√£o da resposta
                response = model.generate_content(full_prompt)
                st.markdown(response.text)
                st.session_state.messages.append({"role": "assistant", "content": response.text})
            except Exception as e:
                st.error(f"Erro na API Gemini: {e}")
                st.write("Dica: Verifica se instalaste a vers√£o mais recente da biblioteca: pip install -U google-generativeai")

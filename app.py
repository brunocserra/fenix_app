import streamlit as st
import pandas as pd
import google.generativeai as genai

# --- CONFIGURA√á√ÉO IA ---
# Chave fornecida pelo utilizador
GENAI_KEY = "AIzaSyCAupRudeVbP7QSSw7v4BDjG0zJ4Y5XE-0"
genai.configure(api_key=GENAI_KEY)

# Configura√ß√£o do Modelo Flash (Escal√£o Gratuito: 15 RPM / 1500 RPD)
generation_config = {
  "temperature": 0.2, # Menor temperatura = Respostas mais t√©cnicas e menos criativas
  "top_p": 0.95,
  "top_k": 64,
  "max_output_tokens": 2048,
}

model = genai.GenerativeModel(
  model_name="gemini-1.5-flash",
  generation_config=generation_config,
)

# --- CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(page_title="IST Planner GPT - Full Context", layout="wide", page_icon="üéì")

@st.cache_data
def load_data():
    try:
        # Carregamento do ficheiro detalhado gerado pelo script anterior
        df = pd.read_csv("planeamento_ist_detalhado_2026.csv", encoding="utf-8-sig")
        df['id_cadeira'] = df['id_cadeira'].astype(str)
        return df
    except Exception as e:
        st.error(f"Erro ao carregar ficheiro CSV: {e}")
        return pd.DataFrame()

df = load_data()

# --- INTERFACE ---
st.title("üöÄ IST Smart Planner & AI Assistant")
st.markdown("---")

# Separa√ß√£o por Tabs: Navega√ß√£o Cl√°ssica vs. Intelig√™ncia Artificial
tab_explorador, tab_chat = st.tabs(["üîç Explorador de Disciplinas", "ü§ñ Assistente IA (Contexto Total)"])

# --- TAB 1: EXPLORADOR MANUAL ---
with tab_explorador:
    st.sidebar.header("Filtros de Procura")
    search = st.sidebar.text_input("Procurar por Nome, Sigla ou ID", "").lower()
    
    # L√≥gica de filtragem r√°pida para a interface manual
    mask = (df['sigla_curso_ref'].str.lower().str.contains(search, na=False) | 
            df['nome_cadeira'].str.lower().str.contains(search, na=False) |
            df['id_cadeira'].str.contains(search, na=False))
    
    df_filtered = df[mask]
    
    if not df_filtered.empty:
        # Seletor formatado com pelicas conforme solicitado no perfil
        df_filtered['label'] = "[" + df_filtered['sigla_curso_ref'] + "] " + df_filtered['nome_cadeira']
        
        escolha = st.selectbox("Selecione uma Unidade Curricular:", ["-- Escolha uma op√ß√£o --"] + sorted(df_filtered['label'].tolist()))
        
        if escolha != "-- Escolha uma op√ß√£o --":
            row = df_filtered[df_filtered['label'] == escolha].iloc[0]
            
            c1, c2 = st.columns([2, 1])
            with c1:
                st.header(row['nome_cadeira'])
                st.subheader("üìñ Programa")
                st.write(row['programa'] if pd.notna(row['programa']) else "N/A")
                st.subheader("üìù M√©todo de Avalia√ß√£o")
                st.info(row['metodo_avaliacao'] if pd.notna(row['metodo_avaliacao']) else "N/A")
            
            with c2:
                st.metric("ECTS", row['ects'])
                st.metric("Per√≠odo", row['periodo'])
                st.write("**üë®‚Äçüè´ Docentes:**")
                for d in str(row['docentes']).split(" | "):
                    st.write(f"- {d}")
                st.link_button("üåê Abrir no F√©nix", row['url_curso'])
    else:
        st.warning("Nenhuma disciplina encontrada.")

# --- TAB 2: ASSISTENTE IA (CHAT) ---
with tab_chat:
    st.header("ü§ñ Conversar com os Dados do IST")
    st.caption("O Gemini Flash est√° a ler o teu ficheiro completo para responder.")

    # Prepara√ß√£o do Contexto para a IA (Enviando a tabela completa sem filtros)
    # Convertemos para Markdown para a IA entender melhor a estrutura de tabela
    contexto_ia = df[['sigla_curso_ref', 'nome_cadeira', 'ects', 'periodo', 'metodo_avaliacao', 'programa']].to_markdown(index=False)

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Exibi√ß√£o das mensagens do hist√≥rico
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Entrada do Chat
    if prompt := st.chat_input("Ex: Quais as cadeiras de Aeroespacial com projetos em grupo no P4?"):
        # Adicionar pergunta do utilizador ao hist√≥rico
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            # Prompt de Sistema para garantir rigor t√©cnico
            full_prompt = f"""
            Contexto: Tu √©s um consultor de planeamento acad√©mico do Instituto Superior T√©cnico.
            Est√°s a analisar os dados do 2¬∫ Semestre de 2025/2026.
            O utilizador √© um Engenheiro Aeroespacial e empres√°rio que valoriza rigor e dados estruturados.
            
            Instru√ß√£o: Responde √† pergunta usando EXCLUSIVAMENTE os dados fornecidos abaixo. 
            Se a resposta envolver f√≥rmulas ou refer√™ncias a colunas, usa pelicas (ex: `nome_cadeira`).
            
            DADOS (CSV/Markdown):
            {contexto_ia}
            
            Pergunta: {prompt}
            """
            
            try:
                # Gerar resposta via API
                response = model.generate_content(full_prompt)
                st.markdown(response.text)
                # Guardar resposta no hist√≥rico
                st.session_state.messages.append({"role": "assistant", "content": response.text})
            except Exception as e:
                st.error(f"Erro na comunica√ß√£o com o Gemini: {e}")
